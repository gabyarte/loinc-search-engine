{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import string as st\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while...\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "dfs = pd.read_excel('../data/loinc_dataset-v2_1.xlsx', sheet_name=None)\n",
    "\n",
    "# Concat the all sheets in a dataframe\n",
    "df = pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the columns\n",
    "df['combined_col'] = df['long_common_name'] \\\n",
    "    + df['component'] \\\n",
    "    + df['system'] \\\n",
    "    + ['property']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning and processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuations from the text\n",
    "def remove_punct(text):\n",
    "    return ''.join([ch for ch in text if ch not in st.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['removed_punc'] = df['combined_col'].apply(remove_punct)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['removed_punc'].str.lower().str.split(r'\\s+')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Remove stopwords. Here, NLTK corpus list is used for a match. However, a customized user-defined \n",
    "    list could be created and used to limit the matches in input text. \n",
    "'''\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['clean_tokens'] = df['tokens'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now annotate each token in a document with its Part-Of-Speech tag (note that tokenized FULL sentences are required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_tag'] = df['tokens'].apply(nltk.pos_tag)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentences to get clean text as input for vectors\n",
    "def return_sentences(tokens):\n",
    "    return \" \".join([word for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['loinc_num'] + ' ' + df['clean_tokens'].apply(return_sentences)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF : Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lemmatized words to Tf-Idf feature vectors\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_vect = tfidf.fit_transform(df['clean_text'])\n",
    "tfidf_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df.drop_duplicates(['loinc_num'])\n",
    "coef = df_d.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.concat([df_d, df_d, df_d], ignore_index=True)\n",
    "df_t['query'] = ''\n",
    "df_t.loc[df_t.index.isin(range(0, coef)), 'query'] = 'glucose in blood'\n",
    "df_t.loc[df_t.index.isin(range(coef, 2 * coef)), 'query'] = 'bilirubin in plasma'\n",
    "df_t.loc[df_t.index.isin(range(2 * coef, 3 * coef)), 'query'] = 'white blood cells count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['query'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['rank'] = np.random.randint(2, size=len(df_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.to_csv('../data/preprocessed_toy_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
